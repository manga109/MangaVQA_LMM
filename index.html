<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="JMMMU-Pro is benchmark for evaluating image-based Japanese multi-discipline multimodal understanding benchmark.">
  <meta name="keywords" content="Japanese Multi-discipline Multimodal Understanding, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>
    JMMMU-Pro: Vibe Benchmark Construction of Image-based Japanese Multi-discipline Multimodal Understanding Benchmark</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/jmmmu_top.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">JMMMU-Pro: Vibe Benchmark Construction of Image-based Japanese Multi-discipline Multimodal Understanding Benchmark</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://atsumiyai.github.io/">Atsuyuki Miyai</a>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shota-onohara/"> Shota Onohara</a>,
            </span>
            <span class="author-block">
              <a href="https://jeonghunbaek.net/5e50812ade674698bdffe7f4eac5c745"> Jeonghun Baek</a>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=CJRhhi0AAAAJ&hl=en">Kiyoharu Aizawa</a>,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">The University of Tokyo</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.01952"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="TBD"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/JMMMU/JMMMU-Pro"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="teaser" style="max-width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Building JMMMU-Pro via Vibe Benchmark Construction. JMMMU-Pro extends
        JMMMU by embedding each question image and text into a single image. To construct JMMMU-Pro,
        we propose Vibe Benchmark Construction, where an image generation model creates questions,
        followed by human verification and prompt refinement to ensure quality. Experiments indicate that
        current open-source LMMs struggle with JMMMU-Pro.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method.
          </p>
          <p>
            Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual–textual understanding through visual perception.
          </p>
          
          <h3 class="title is-5">1. We introduce <strong>JMMMU-Pro</strong> (<strong><em>Japanese MMMU-Pro</em></strong>)</h3>
          <p>
            Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual–textual understanding through visual perception.
          </p>

          <h3 class="title is-5">2. We introduce <strong>Vibe Benchmark Construction</strong></h3>
          <p>
            To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality.
            By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to render clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs.
          </p>

          <h3 class="title is-5">3. Findings</h3>
          <p>
            Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community.
            We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Benchmark Construction. -->
      <div class="column">
        <div class="content has-text-left">
          <h2 class="title is-3 has-text-centered">Benchmark Construction</h2>

          <!-- Image and caption -->
          <h3 class="title is-4 has-text-centered" style="margin-top: 0.5rem; margin-bottom: 2.0rem;">
            Vibe Benchmark Construction
          </h3>
          <p>
            Vibe Benchmark Construction is a methodology in which an image generation model plays the primary role in producing the VQA problem images, while humans only verify the outputs and adjust the prompts when necessary to ensure quality.
          </p>
          <p>
            Although previous VQA benchmarks have used synthetic images generated by image generation models, these models have played only a supplementary role, producing just the visual part while the question text still had to be created separately by humans or by an LMM, incurring additional cost.
          </p>
          <p>
            In contrast, the key distinction of our proposed Vibe Benchmark Construction is that the entire VQA creation process is carried out by the image generation model, with humans intervening solely for verification and prompt refinement.
          </p>
          <p>
            This paradigm is particularly effective for image-based VQA, where humans cannot easily edit content directly inside the image in the same way as text-based QA. By letting the model handle generation and restricting human effort to <em>adjusting the prompt until a satisfactory image is produced</em>, the method enables efficient and scalable construction of benchmarks, especially in domains like image-based VQA, where dataset creation is difficult. A more detailed comparison with existing work is provided in the related work section.
          </p>
          
          <img src="./static/images/vibe_construction.png" alt="Vibe Benchmark Construction" style="max-width: 100%; height: auto; margin-bottom: 2rem;">

        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Dataset Statistics. -->
      <div class="column">
        <div class="content has-text-centered">
          <h2 class="title is-3">Experimental Results</h2>
          <p>
            The following figures illustrate the overall results on JMMMU-Pro.
          </p>

          <!-- First image and caption -->
          <h3 class="title is-4" style="margin-top: 0.1rem; margin-bottom: 0.3rem;">Overall Results</h3>
          <img src="./static/images/main_results.png" alt="Overall Results" style="max-width: 100%; height: auto; margin-bottom: 0.3rem;">

          <div class="content has-text-left">
            <h4 class="title is-5">F1. All open-source LMMs struggle significantly on JMMMU-Pro.</h4>
            <p>
              As shown in the results table, open-source LMMs perform poorly on JMMMU-Pro, with the best model, Qwen3-VL-8B, achieving only 45.83, indicating substantial room for improvement. Furthermore, nine models perform less than 32%, close to random guessing.
            </p>
            <p>
              These results highlight that JMMMU-Pro poses a challenging and valuable benchmark for evaluating and advancing open-source LMMs.
            </p>

            <h4 class="title is-5">F2. Most open-source LMMs exhibit a significant performance drop compared to JMMMU.</h4>
            <p>
              As shown in the results table, most open-source LMMs, except for Qwen2.5-VL-7B, show a substantial decline in accuracy on JMMMU-Pro relative to JMMMU.
            </p>
            <p>
              Moreover, when we compare the CS and CA subsets, we find that models with a clear performance gap between the JMMMU's two subsets are similarly low on both in JMMMU-Pro. This suggests that their weakness lies in a fundamental lack of vision-side understanding, rather than in the type of question.
            </p>
            <p>
              These results demonstrate that JMMMU-Pro provides valuable feedback to model developers when used in comparison with JMMMU.
            </p>

            <h4 class="title is-5">F3. Closed-source LMMs achieve substantially higher performance on JMMMU-Pro, revealing a serious gap relative to open-source models.</h4>
            <p>
              As shown in the results table, closed-source LMMs obtain notably high scores on JMMMU-Pro. This indicates that these models already possess the ability to seamlessly integrate visual and textual information and interpret them through visual perception.
            </p>
            <p>
              Importantly, the strong performance of closed-source models does not diminish the value of JMMMU-Pro. Instead, it highlights the crucial role of JMMMU-Pro as a benchmark for guiding the development of open-source LMMs. Given the considerable performance gap between closed-source and open-source LMMs, reducing this gap is an essential goal for the community.
            </p>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-left">
          <h2 class="title is-3 has-text-centered">Chain-of-Thought Analysis</h2>

          <!-- Image and caption -->
          <h3 class="title is-4 has-text-centered" style="margin-top: 0.5rem; margin-bottom: 2.0rem;">
            CoT Prompting Effectiveness
          </h3>
          <p>
            We examine the effectiveness of Chain-of-Thought (CoT) prompting on JMMMU-Pro and JMMMU. The results are shown in the figure below. These results indicate that the effectiveness of CoT varies depending on the model and the evaluation setting for both JMMMU-Pro and JMMMU.
          </p>
          <p>
            For example, on JMMMU-Pro, 7 out of the 12 LMMs achieve higher performance with CoT prompting, whereas on JMMMU, only 3 models benefit from CoT. Moreover, when examined on a per-model basis, LMMs such as Pangea-7B, LLaVA-OV-1.5-8B, InternVL2.5-8B, and Sarashina2.2-V-3B show different prompt preferences between JMMMU and JMMMU-Pro.
          </p>
          <p>
            These findings suggest that optimal prompting strategies must be tailored to each model and each task, rather than relying on a single prompting approach across settings.
          </p>
          
          <img src="./static/images/cot_analysis.png" alt="CoT Analysis" style="max-width: 100%; height: auto; margin-bottom: 2rem;">

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-left">
          <h2 class="title is-3 has-text-centered">OCR Analysis</h2>

          <!-- Image and caption -->
          <h3 class="title is-4 has-text-centered" style="margin-top: 0.5rem; margin-bottom: 2.0rem;">
            OCR Performance Correlation
          </h3>
          <p>
            We hypothesize that the primary cause of performance degradation on JMMMU-Pro is the inability of current LMMs to perform Japanese Optical Character Recognition (OCR). To examine this hypothesis, we compute the correlation between OCR performance and JMMMU-Pro accuracy across several LMMs.
          </p>
          <p>
            The results are shown in the figure below. The correlation coefficient between OCR accuracy and JMMMU-Pro accuracy is 0.593. As illustrated in the figure, there is indeed a positive correlation between the two. However, high OCR ability does not necessarily translate directly into high JMMMU-Pro accuracy. For example, while Heron-NVILA and Sarashina2.2-V are comparable for OCR performance, the performance for JMMMU-Pro differs a lot.
          </p>
          <p>
            This indicates that solving JMMMU-Pro demands not only strong OCR capabilities, but also the ability to interpret and reason over language and vision in an integrated manner through visual perception.
          </p>
          
          <img src="./static/images/ocr_analysis.png" alt="OCR Analysis" style="max-width: 100%; height: auto; margin-bottom: 2rem;">

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-left">
          <h2 class="title is-3 has-text-centered">Samples</h2>

          <!-- Image and caption -->
          <h3 class="title is-4 has-text-centered" style="margin-top: 0.5rem; margin-bottom: 2.0rem;">
            JMMMU-Pro Samples
          </h3>
          <p>
            We present samples from JMMMU-Pro in the figure below. As shown in this figure, JMMMU-Pro includes images with a wide variety of backgrounds, reflecting the diversity in real-world scenarios.
          </p>
          
          <img src="./static/images/samples.png" alt="JMMMU-Pro Samples" style="max-width: 100%; height: auto; margin-bottom: 2rem;">

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{miyai2025jmmmu-pro,
  author    = {Miyai, Atsuyuki and Onohara, Shota and Baek, Jeonghun and Aizawa, Kiyoharu},
  title     = {JMMMU-Pro: Vibe Benchmark Construction of Image-based Japanese Multi-discipline Multimodal Understanding Benchmark},
  journal   = {TBD},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
